{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category  rating  \\\n",
       "0  Home_and_Kitchen     5.0   \n",
       "1  Home_and_Kitchen     5.0   \n",
       "2  Home_and_Kitchen     5.0   \n",
       "3  Home_and_Kitchen     1.0   \n",
       "4  Home_and_Kitchen     5.0   \n",
       "\n",
       "                                                text  label  \n",
       "0  Love this!  Well made, sturdy, and very comfor...      1  \n",
       "1  love it, a great upgrade from the original.  I...      1  \n",
       "2  This pillow saved my back. I love the look and...      1  \n",
       "3  Missing information on how to use it, but it i...      1  \n",
       "4  Very nice set. Good quality. We have had the s...      1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fake_reviews_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-50: 23764 reviews\n",
      "50-100: 8139 reviews\n",
      "100-150: 3435 reviews\n",
      "150-200: 2074 reviews\n",
      "200-250: 1432 reviews\n",
      "250-300: 1101 reviews\n",
      "300-350: 513 reviews\n",
      "350-400: 61 reviews\n",
      "400-450: 3 reviews\n",
      "450-500: 2 reviews\n",
      "500-550: 2 reviews\n",
      "550-600: 0 reviews\n",
      "600-650: 0 reviews\n",
      "650-700: 0 reviews\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('fake_reviews_dataset.csv')\n",
    "\n",
    "# Assuming the text column is named 'text'\n",
    "texts = df['text'].astype(str).tolist()\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Calculate the length of each sequence\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "# Define buckets (0-50, 50-100, ..., 650-700)\n",
    "buckets = np.arange(0, 750, 50)\n",
    "\n",
    "# Count the number of sequences in each bucket\n",
    "bucket_counts = Counter(np.digitize(sequence_lengths, buckets))\n",
    "\n",
    "# Print the number of reviews in each bucket\n",
    "for i in range(len(buckets) - 1):\n",
    "    print(f\"{buckets[i]}-{buckets[i+1]}: {bucket_counts[i+1]} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split_data(df):\n",
    "    # Load stop words and initialize stemmer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Preprocessing function: remove stop words and apply stemming\n",
    "    def preprocess_text(text):\n",
    "        words = text.split()\n",
    "        filtered_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X = df['text']  # Features (text data)\n",
    "    y = df['label']  # Labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Apply preprocessing to the training and testing sets separately\n",
    "    X_train = X_train.apply(preprocess_text)\n",
    "    X_test = X_test.apply(preprocess_text)\n",
    "\n",
    "    # Tokenize and pad sequences\n",
    "    max_words = 10000  # Hardcoded maximum number of words\n",
    "    max_len = 100      # Hardcoded maximum sequence length\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Return the processed datasets and tokenizer\n",
    "    return X_train_pad, X_test_pad, y_train, y_test, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (32420, 100) (32420,)\n",
      "Test shape: (8106, 100) (8106,)\n"
     ]
    }
   ],
   "source": [
    "# Example usage for preprocessing\n",
    "X_train, X_test, y_train, y_test, tokenizer = preprocess_and_split_data(df)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training set: [16201 16219]\n",
      "Class distribution in testing set: [4031 4075]\n",
      "Class percentages in training set: [49.97223936 50.02776064]\n",
      "Class percentages in testing set: [49.7285961 50.2714039]\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution in training and testing sets\n",
    "import numpy as np\n",
    "\n",
    "# Calculate class distribution\n",
    "train_class_distribution = np.bincount(y_train)\n",
    "test_class_distribution = np.bincount(y_test)\n",
    "\n",
    "# Print class distribution\n",
    "print(\"Class distribution in training set:\", train_class_distribution)\n",
    "print(\"Class distribution in testing set:\", test_class_distribution)\n",
    "\n",
    "# Calculate percentages for better understanding\n",
    "train_class_percentage = train_class_distribution / len(y_train) * 100\n",
    "test_class_percentage = test_class_distribution / len(y_test) * 100\n",
    "\n",
    "print(\"Class percentages in training set:\", train_class_percentage)\n",
    "print(\"Class percentages in testing set:\", test_class_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.5115 - loss: 0.6936 - val_accuracy: 0.6798 - val_loss: 0.6140\n",
      "Epoch 2/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.6763 - loss: 0.6089 - val_accuracy: 0.6521 - val_loss: 0.5851\n",
      "Epoch 3/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.6853 - loss: 0.5646 - val_accuracy: 0.7565 - val_loss: 0.5250\n",
      "Epoch 4/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.7619 - loss: 0.5201 - val_accuracy: 0.7850 - val_loss: 0.4611\n",
      "Epoch 5/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.7900 - loss: 0.4661 - val_accuracy: 0.7822 - val_loss: 0.4888\n",
      "Epoch 6/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 27ms/step - accuracy: 0.7926 - loss: 0.4636 - val_accuracy: 0.8105 - val_loss: 0.4353\n",
      "Epoch 7/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.8251 - loss: 0.4031 - val_accuracy: 0.8282 - val_loss: 0.3966\n",
      "Epoch 8/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8790 - loss: 0.3066 - val_accuracy: 0.8681 - val_loss: 0.3106\n",
      "Epoch 9/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9122 - loss: 0.2182 - val_accuracy: 0.8780 - val_loss: 0.2932\n",
      "Epoch 10/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.9331 - loss: 0.1735 - val_accuracy: 0.8842 - val_loss: 0.3005\n",
      "Epoch 11/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9430 - loss: 0.1516 - val_accuracy: 0.8831 - val_loss: 0.2968\n",
      "Epoch 12/20\n",
      "\u001b[1m811/811\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9512 - loss: 0.1272 - val_accuracy: 0.8848 - val_loss: 0.3050\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8802 - loss: 0.3010\n",
      "Test Accuracy: 0.8797187209129333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "Prediction: Fake Review\n"
     ]
    }
   ],
   "source": [
    "# Example of how to use the preprocessed data for a simple LSTM model\n",
    "\n",
    "# Simple LSTM model for testing\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),  # Smaller embedding dimension for simplicity\n",
    "    LSTM(32),  # Fewer units in the LSTM layer\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,          # Stop training after 3 epochs with no improvement\n",
    "    restore_best_weights=True  # Restore the weights of the best epoch\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,  # Set a high number of epochs; early stopping will terminate earlier if needed\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Predict if a new review is fake or real\n",
    "def predict_review(review, tokenizer, max_len=100):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    words = review.split()\n",
    "    filtered_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "    preprocessed_review = ' '.join(filtered_words)\n",
    "    review_seq = tokenizer.texts_to_sequences([preprocessed_review])  # Tokenize the review\n",
    "    review_pad = pad_sequences(review_seq, maxlen=max_len, padding='post')  # Pad the sequence\n",
    "    prediction = model.predict(review_pad)\n",
    "    return \"Fake Review\" if prediction[0] > 0.5 else \"Real Review\"\n",
    "\n",
    "# Example usage\n",
    "example_review = \"This product is amazing and works perfectly!\"\n",
    "print(\"Prediction:\", predict_review(example_review, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
